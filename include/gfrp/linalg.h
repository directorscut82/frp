#ifndef _GFRP_LINALG_H__
#define _GFRP_LINALG_H__
#define _USE_MATH_DEFINES
#include "gfrp/util.h"
#include "x86intrin.h"

#ifdef __AVX2__
#define VECTOR_WIDTH (32ul)
#elif __SSE2__
#define VECTOR_WIDTH (16ul)
#else
#define VECTOR_WIDTH (8ul)
#endif

namespace gfrp { namespace linalg {

template<typename MatrixKind1, typename MatrixKind2>
void gram_schmidt(const MatrixKind1 &a, MatrixKind2 &b, int flags) {
    b = a;
    gram_schmidt(b, flags);
}

enum GramSchmitFlags {
    FLIP = 1,
    ORTHONORMALIZE = 2,
    RESCALE_TO_GAUSSIAN = 4
};

template<typename ValueType>
void mempluseq(ValueType *data, size_t nelem, ValueType val) {
    throw std::runtime_error("Not implemented.");
}

template<>
void mempluseq<float>(float *data, size_t nelem, float val) {

#if _FEATURE_AVX512F
#define load_fn _mm512_loadu_ps
#define store_fn _mm512_storeu_ps
#define add_fn _mm512_add_ps
    using ValType = __m512;
    const __m512 vval(__m512_set1_ps(val));
#elif __AVX2__
#define load_fn _mm256_loadu_ps
#define store_fn _mm256_storeu_ps
#define add_fn _mm256_add_ps
    using ValType = __m256;
    const __m256 vval(_mm256_set1_ps(val));
#elif __SSE2__
#define load_fn _mm_loadu_ps
#define store_fn _mm_storeu_ps
#define add_fn _mm_add_ps
    using ValType = __m128;
    const __m128 vval(_mm_set1_ps(val));
#else
    while(nelem--) *data += val;
    return;
#endif
#if _FEATURE_AVX512F || __AVX2__ || __SSE2__
    while(nelem >= (sizeof(ValType) / sizeof(float))) {
        store_fn(data, add_fn(load_fn(data), vval)); // *data += vval
        nelem -= (sizeof(ValType) / sizeof(float)); // Move ahead and skip that many elements
        data += (sizeof(ValType) / sizeof(float));
    }
    while(nelem--) *data++ += val;
#undef load_fn
#undef add_fn
#undef store_fn
#endif
}
template<>
void mempluseq<double>(double *data, size_t nelem, double val) {
#if _FEATURE_AVX512F
#define load_fn _mm512_loadu_pd
#define store_fn _mm512_storeu_pd
#define add_fn _mm512_add_pd
    using ValType = __m512d;
    const __m512d vval(__m512_set1_pd(val));
#elif __AVX2__
#define load_fn _mm256_loadu_pd
#define store_fn _mm256_storeu_pd
#define add_fn _mm256_add_pd
    using ValType = __m256d;
    const __m256d vval(_mm256_set1_pd(val));
#elif __SSE2__
#define load_fn _mm_loadu_pd
#define store_fn _mm_storeu_pd
#define add_fn _mm_add_pd
    using ValType = __m128d;
    const __m128d vval(_mm_set1_pd(val));
#else
    while(nelem--) *data += val;
    return;
#endif
#if _FEATURE_AVX512F || __AVX2__ || __SSE2__
    while(nelem >= (sizeof(ValType) / sizeof(float))) {
        store_fn(data, add_fn(load_fn(data), vval)); // *data += vval
        nelem -= (sizeof(ValType) / sizeof(float)); // Move ahead and skip that many elements
        data += (sizeof(ValType) / sizeof(float));
    }
    while(nelem--) *data++ += val;
#undef load_fn
#undef add_fn
#undef store_fn
#endif
}


template<typename MatrixType, typename ValueType,
         typename=std::enable_if_t<std::is_arithmetic<ValueType>::value>>
MatrixType &operator+=(MatrixType &in, ValueType val) {
    if constexpr(blaze::IsMatrix<MatrixType>::value) {
        if constexpr(blaze::IsSparseMatrix<MatrixType>::value) {
            for(size_t i(0); i < in.rows(); ++i) {
                for(auto it(in.begin(i)), eit(in.end(i)); it != eit; ++it) {
                    it->value() += val;
                }
            }
        } else {
            if(size_t(&in(0, 1) - &in(0, 0)) == 1) {
                for(size_t i(0); i < in.rows(); ++i) {
                    mempluseq(&in(i, 0), in.columns(), val);
                }
            } else {
                for(size_t i(0); i < in.columns(); ++i) {
                    mempluseq(&in(0, i), in.rows(), val);
                }
            }
        }
    } else {
        if constexpr(blaze::IsSparseVector<MatrixType>::value) {
            for(auto it(in.begin()), eit(in.end()); it != eit; ++it) it->value() += val;
        } else {
            if(size_t(&in[0] - &in[1]) == 1) {
                mempluseq(&in[0], in.size(), val);
            } else {
                for(auto &el: in) el = val;
            }
        }
    }
    return in;
}

template<typename MatrixType, typename ValueType,
         typename=std::enable_if_t<std::is_arithmetic<ValueType>::value>>
MatrixType &operator-=(MatrixType &in, ValueType val) {
    return in += -val;
}


/*
 Am I doing this RESCALE_TO_GAUSSIAN right?
 Realizations of Gort can be generated by, for example, performing a Gram-Schmidt process
 on the rows of G to obtain a set of orthonormal rows,
 and then randomly independently scaling each row so that marginally
 it has the distribution of a Gaussian vector.
 */

template<typename MatrixKind>
void gram_schmidt(MatrixKind &b, int flags=(FLIP & ORTHONORMALIZE)) {
    using FloatType = typename MatrixKind::ElementType;
    if(flags & FLIP) {
        blaze::DynamicVector<FloatType> inv_unorms(b.columns());
        FloatType tmp;
        for(size_t i(0), ncolumns(b.columns()); i < ncolumns; ++i) {
            auto icolumn(column(b, i));
            for(size_t j(0); j < i; ++j) {
                auto jcolumn(column(b, j));
                icolumn -= jcolumn * dot(icolumn, jcolumn) * inv_unorms[j];
            }
            if((tmp = dot(icolumn, icolumn)) == 0.0) {
            }
            inv_unorms[i] = tmp ? tmp: std::numeric_limits<decltype(tmp)>::max();
#if !NDEBUG
            if(tmp ==std::numeric_limits<decltype(tmp)>::max())
                std::fprintf(stderr, "Warning: norm of column %zu (0-based) is 0.0\n", i);
#endif
        }
        if(flags & ORTHONORMALIZE)
            for(size_t i(0), ncolumns(b.columns()); i < ncolumns; ++i)
                column(b, i) *= inv_unorms[i];
        if(flags & RESCALE_TO_GAUSSIAN) {
            for(size_t i = 0, ncolumns = b.columns(); i < ncolumns; ++i) {
                auto mcolumn(column(b, i));
                auto meanvarpair(meanvar(mcolumn)); // mean.first = mean, mean.second = var
                const auto invsqrt(1./std::sqrt(meanvarpair.second * b.rows()));
                mcolumn -= meanvarpair.first;
                mcolumn *= invsqrt;
            }
        }
    } else {
        blaze::DynamicVector<FloatType> inv_unorms(b.rows());
        for(size_t i(0), nrows(b.rows()); i < nrows; ++i) {
            auto irow(row(b, i));
            for(size_t j(0); j < i; ++j) {
                auto jrow(row(b, j));
                irow -= jrow * dot(irow, jrow) * inv_unorms[j];
            }
            auto tmp(1./dot(irow, irow));
            inv_unorms[i] = tmp ? tmp: std::numeric_limits<decltype(tmp)>::max();
        }
        if(flags & ORTHONORMALIZE)
            for(size_t i(0), nrows(b.rows()); i < nrows; ++i)
                row(b, i) *= inv_unorms[i];
        if(flags & RESCALE_TO_GAUSSIAN) {
            const size_t nrows(b.rows());
            for(size_t i = 0; i < nrows; ++i) {
                auto mrow(row(b, i));
                const auto meanvarpair(meanvar(mrow)); // mean.first = mean, mean.second = var
                mrow -= meanvarpair.first;
                mrow *= 1./std::sqrt(meanvarpair.second * b.columns());
            }
        }
    }
}

template<typename MatrixKind>
decltype(auto) frobnorm(const MatrixKind &mat) {
    using FloatType = typename MatrixKind::ElementType;
    FloatType ret(0.);
    for(size_t i(0); i < mat.rows(); ++i)
        ret += dot(row(mat, i), row(mat, i));
    return ret;
}

template<typename FloatType>
constexpr inline auto ndball_surface_area(std::size_t nd, FloatType r) {
    // http://scipp.ucsc.edu/~haber/ph116A/volume_11.pdf
    // In LaTeX notation: $\frac{2\Pi^{\frac{n}{2}}R^{n-1}}{\Gamma(frac{n}{2})}$
    nd >>= 1;
    return 2. * std::pow(static_cast<FloatType>(M_PI), nd) / tgamma(nd) * std::pow(r, nd - 1);
}


}} // namespace gfrp::linalg

#endif // #ifnef _GFRP_LINALG_H__
